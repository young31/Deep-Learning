# Tranfer Learning(전이 학습)

이미지에 대한 분석을 할 때 가장 유용한 개념이다.  
이미 훈련된 모형을 다른 문제에 적용시키는 개념으로 자료의 한계로 인해 모형을 충분히 훈련시키기 어려운 상황에서 강력한 대안이된다.


단계는 아래와 같다.
1. 사용할 모형을 정의한다.
2. 원본 모형의 FC와 출력층을 제거한다.
3. 2에서 제거한 층에 대해 새로운 문제에 적합한 계층으로 바꾼다.
4. 훈련 계층을 동결시키고 훈련한다.  
5. 가능하다면 동결시킨 층의 일부를 풀어 fine tuning한다.

이미지 처리와 관련된 모형으로 쉽게 사용할 수 있는 모형으로는
- VGG16, VGG19
- Inception V3
- Resnet50/senet50
등이 있다.([참고](https://keras.io/applications/#applications))
위 모형들은 보통 매우 깊어 성능면으로는 우월하지만 직접 훈련하기는 어렵다는 특징이 있다.  
대표적인 모형들 외에도 직접 만들어서 사용한 모형을 저장해 두었다가 재활용 할 수도 있을 것이다.  

구글의 연구에 의하면 자료의 수가 많아질수록 그 log값에 비례하여 모델 성능이 좋아진다.   
전이 학습의 경우에도 비슷한 실험들이 수행되었고 유사한 결과를 얻을 수 있었다.  


실제 프로젝트를 진행하면서 가장 유용했던 개념이다.  
망을 너무 얕게 구축하면 특징을 제대로 추출하지 못하였어서 망을 깊게 구축할 필요가 있었다.  
그러나 작업환경의 문제로 너무 큰 모형은 훈련시킬 수 없는 문제가 발생했다.(OOM에러).  
이런 상황에서 미리 훈련된 모형(다행히 원본정의역이 존재하였다.)을 불러와 동결시켜 학습시킬 양은 줄이면서도 망을 활용할 수 있었다.
